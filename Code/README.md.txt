You can download the CoNLL 2003 dataset at this URL: https://deepai.org/dataset/conll-2003-english
Please adjust your file directory accordingly - where the python files are run from and the data directory parameter --data_dir

For tuning the hyperparameters of TAVAT, please change the values of these parameters: --adv_lr  --adv_max_norm  --adv_steps 
You can train and test the model using this command:
CUDA_VISIBLE_DEVICES=0 python ./code/DL_submission/token_vat.py --model_type bert --model_name_or_path bert-base-cased --do_lower_case --learning_rate 5e-5   --do_train  --do-eval --evaluate-during-training --do_predict --task_name mnli --data_dir 'data/conll2003'  --output_dir SOTATavatLearnParamHiddentest --overwrite_output_dir --max_seq_length 256 --save_steps 750 --logging_steps 150 --evaluate_during_training --per_gpu_train_batch_size 8 --warmup_steps 0 --num_train_epochs 10 --adv_lr 5e-2 --adv_init_mag 2e-1 --adv_max_norm 5e-1 --adv_steps 2 --vocab_size 28996 --hidden_size 768 --adv_train 1 --gradient_accumulation_steps 1 --max_steps -1

To perform Initialization of Token-Level Perturbation experiment, run this command:
CUDA_VISIBLE_DEVICES=0 python ./code/DL_submission/token_vat_init_vocab.py --model_type bert --model_name_or_path bert-base-cased --do_lower_case --learning_rate 5e-5   --do_train  --do-eval --evaluate-during-training --do_predict --task_name mnli --data_dir 'data/conll2003'  --output_dir SOTATavatLearnParamHiddentest --overwrite_output_dir --max_seq_length 256 --save_steps 750 --logging_steps 150 --evaluate_during_training --per_gpu_train_batch_size 8 --warmup_steps 0 --num_train_epochs 10 --adv_lr 5e-2 --adv_init_mag 2e-1 --adv_max_norm 5e-1 --adv_steps 2 --vocab_size 28996 --hidden_size 768 --adv_train 1 --gradient_accumulation_steps 1 --max_steps -1

To perform Removing Token-Level Constraint in the Norm experiment, run this command:
CUDA_VISIBLE_DEVICES=0 python ./code/DL_submission/token_vat_removing_constraint.py --model_type bert --model_name_or_path bert-base-cased --do_lower_case --learning_rate 5e-5  --do_train  --do-eval --evaluate-during-training  --do_predict --task_name mnli --data_dir 'data/conll2003'  --output_dir SOTATavatLearnParamHiddentest --overwrite_output_dir --max_seq_length 256 --save_steps 750 --logging_steps 150 --evaluate_during_training --per_gpu_train_batch_size 8 --warmup_steps 0 --num_train_epochs 10 --adv_lr 5e-2 --adv_init_mag 2e-1 --adv_max_norm 5e-1 --adv_steps 2 --vocab_size 28996 --hidden_size 768 --adv_train 1 --gradient_accumulation_steps 1 --max_steps -1

To perform Nuclear Norm for Standardization in place of Frobenius Norm experiment, run this command:
CUDA_VISIBLE_DEVICES=0 python ./code/DL_submission/token_vat_nuclear_norm.py --model_type bert --model_name_or_path bert-base-cased --do_lower_case --learning_rate 5e-5   --do_train  --do-eval --evaluate-during-training --do_predict --task_name mnli --data_dir 'data/conll2003'  --output_dir SOTATavatLearnParamHiddentest --overwrite_output_dir --max_seq_length 256 --save_steps 750 --logging_steps 150 --evaluate_during_training --per_gpu_train_batch_size 8 --warmup_steps 0 --num_train_epochs 10 --adv_lr 5e-2 --adv_init_mag 2e-1 --adv_max_norm 5e-1 --adv_steps 2 --vocab_size 28996 --hidden_size 768 --adv_train 1 --gradient_accumulation_steps 1 --max_steps -1

To perform Effect of Noise on the Word Embeddings with Gaussian noise experiment, adjust distribution shape at line 520 in code and run this command:
CUDA_VISIBLE_DEVICES=0 python ./code/DL_submission/token_vat_noise.py --model_type bert --model_name_or_path bert-base-cased --do_lower_case --learning_rate 5e-5   --do_train  --do-eval --evaluate-during-training --do_predict --task_name mnli --data_dir 'data/conll2003'  --output_dir SOTATavatLearnParamHiddentest --overwrite_output_dir --max_seq_length 256 --save_steps 750 --logging_steps 150 --evaluate_during_training --per_gpu_train_batch_size 8 --warmup_steps 0 --num_train_epochs 10 --adv_lr 5e-2 --adv_init_mag 2e-1 --adv_max_norm 5e-1 --adv_steps 2 --vocab_size 28996 --hidden_size 768 --adv_train 1 --gradient_accumulation_steps 1 --max_steps -1

To perform Effect of Noise on the Word Embeddings with Bernoulli noise experiment, adjust p value at line 1128 in code and run this command:
CUDA_VISIBLE_DEVICES=0 python ./code/DL_submission/token_vat_ber_noise.py --model_type bert --model_name_or_path bert-base-cased --do_lower_case --learning_rate 5e-5   --do_train  --do-eval --evaluate-during-training --do_predict --task_name mnli --data_dir 'data/conll2003'  --output_dir SOTATavatLearnParamHiddentest --overwrite_output_dir --max_seq_length 256 --save_steps 750 --logging_steps 150 --evaluate_during_training --per_gpu_train_batch_size 8 --warmup_steps 0 --num_train_epochs 10 --adv_lr 5e-2 --adv_init_mag 2e-1 --adv_max_norm 5e-1 --adv_steps 2 --vocab_size 28996 --hidden_size 768 --adv_train 1 --gradient_accumulation_steps 1 --max_steps -1

To perform  Learning the Function for Combining the Embeddings and Pertubations experiment, adjust the theta parameter intialization shape at line 22 in code file bert_models_tavat and run this command:
CUDA_VISIBLE_DEVICES=0 python ./code/DL_submission/token_vat_custom_comb.py --model_type bert --model_name_or_path bert-base-cased --do_lower_case --learning_rate 5e-5   --do_train  --do-eval --evaluate-during-training --do_predict --task_name mnli --data_dir 'data/conll2003'  --output_dir SOTATavatLearnParamHiddentest --overwrite_output_dir --max_seq_length 256 --save_steps 750 --logging_steps 150 --evaluate_during_training --per_gpu_train_batch_size 8 --warmup_steps 0 --num_train_epochs 10 --adv_lr 5e-2 --adv_init_mag 2e-1 --adv_max_norm 5e-1 --adv_steps 2 --vocab_size 28996 --hidden_size 768 --adv_train 1 --gradient_accumulation_steps 1 --max_steps -1